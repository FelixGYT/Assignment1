# Homework-week5
## 前三题必做

## 1. 不考虑多头的原因，self-attention中词向量不乘QKV（Wq、Wk、Wv）参数矩阵，会有什么问题？
Self-Attention的核心是用文本中的其它词来增强目标词的语义表示，从而更好的利用上下文的信息。

self-attention中，sequence中的每个词都会和sequence中的每个词做点积去计算相似度，也包括这个词本身。

如果不乘QKV参数矩阵，那这个词对应的q,k,v就是完全一样的。

在相同量级的情况下，qi与ki点积的值会是最大的（可以从“两数和相同的情况下，两数相等对应的积最大”类比过来）。

那在softmax后的加权平均中，该词本身所占的比重将会是最大的，使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示。

而乘以QKV参数矩阵，会使得每个词的q,k,v都不一样，能很大程度上减轻上述的影响。

当然，QKV参数矩阵也使得多头，类似于CNN中的多核，去捕捉更丰富的特征/信息成为可能。

## 2. Transformer的点积模型做缩放的原因是什么？
较大的输入会使反向梯度变小，所以使用dk的根号来缩放的话可以使方差控制为1，有效的控制了梯度变小的问题。

## 3. Self-Attention 的时间复杂度是怎么计算的？为多少？
复杂度： O(n^2*d), n是序列的长度，d是embedding的维度
相似度的计算为O(n^2*d)
softmax的复杂度为O(n^2)
加权平均也为O(n^2*d)
因此 复杂度为 O(n^2*d)


## 附加思考题（可做可不做）：

根据问题3求的计算复杂度可以看出，输入序列长度过长会造成计算量太大，那你有什么的想法从结构上改进么？









